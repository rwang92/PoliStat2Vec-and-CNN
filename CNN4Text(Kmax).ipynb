{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bGV2j-qb_RN3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import urllib\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "class TarDataset(data.Dataset):\n",
    "\n",
    "  \n",
    "\n",
    "    @classmethod\n",
    "\n",
    "    def download_or_unzip(cls, root):\n",
    "\n",
    "        path = os.path.join(root, cls.dirname)\n",
    "\n",
    "        if not os.path.isdir(path):\n",
    "\n",
    "            tpath = os.path.join(root, cls.filename)\n",
    "\n",
    "            if not os.path.isfile(tpath):\n",
    "\n",
    "                print('downloading')\n",
    "\n",
    "                urllib.request.urlretrieve(cls.url, tpath)\n",
    "\n",
    "            with tarfile.open(tpath, 'r') as tfile:\n",
    "\n",
    "                print('extracting')\n",
    "\n",
    "                tfile.extractall(root)\n",
    "\n",
    "        return os.path.join(path, '')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MR(TarDataset):\n",
    "\n",
    "\n",
    "\n",
    "    url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "\n",
    "    filename = 'rt-polaritydata.tar'\n",
    "\n",
    "    dirname = 'rt-polaritydata'\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def sort_key(ex):\n",
    "\n",
    "        return len(ex.text)\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "\n",
    "        \n",
    "\n",
    "        def clean_str(string):\n",
    "\n",
    "\n",
    "            string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "\n",
    "            string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "\n",
    "            string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "\n",
    "            string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "\n",
    "            string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "\n",
    "            string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "\n",
    "            string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "\n",
    "            string = re.sub(r\",\", \" , \", string)\n",
    "\n",
    "            string = re.sub(r\"!\", \" ! \", string)\n",
    "\n",
    "            string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "\n",
    "            string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "\n",
    "            string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "\n",
    "            string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "            return string.strip()\n",
    "\n",
    "\n",
    "\n",
    "        text_field.preprocessing = data.Pipeline(clean_str)\n",
    "\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "\n",
    "\n",
    "\n",
    "        if examples is None:\n",
    "\n",
    "            path = self.dirname if path is None else path\n",
    "\n",
    "            examples = []\n",
    "\n",
    "            with open(os.path.join(path, 'rt-polarity.neg'), errors='ignore') as f:\n",
    "\n",
    "                examples += [\n",
    "\n",
    "                    data.Example.fromlist([line, 'negative'], fields) for line in f]\n",
    "\n",
    "            with open(os.path.join(path, 'rt-polarity.pos'), errors='ignore') as f:\n",
    "\n",
    "                examples += [\n",
    "\n",
    "                    data.Example.fromlist([line, 'positive'], fields) for line in f]\n",
    "\n",
    "        super(MR, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "\n",
    "    def splits(cls, text_field, label_field, dev_ratio=.1, shuffle=True, root='.', **kwargs):\n",
    "\n",
    "        \n",
    "\n",
    "        path = cls.download_or_unzip(root)\n",
    "\n",
    "        examples = cls(text_field, label_field, path=path, **kwargs).examples\n",
    "\n",
    "        if shuffle: random.shuffle(examples)\n",
    "\n",
    "        dev_index = -1 * int(dev_ratio*len(examples))\n",
    "\n",
    "\n",
    "\n",
    "        return (cls(text_field, label_field, examples=examples[:dev_index]),\n",
    "\n",
    "                cls(text_field, label_field, examples=examples[dev_index:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bvQKst2r_Sda"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "\n",
    "    \n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        super(CNN_Text, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        V = args.embed_num\n",
    "\n",
    "        D = args.embed_dim\n",
    "\n",
    "        C = args.class_num\n",
    "\n",
    "        Ci = 1\n",
    "\n",
    "        Co = args.kernel_num\n",
    "\n",
    "        Ks = args.kernel_sizes\n",
    "        \n",
    "\n",
    "       \n",
    "        self.embed = nn.Embedding(V, D)\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co*args.kmax, 256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, C)\n",
    "                \n",
    "        \n",
    "    def kmax_pooling(self, x, dim, k):\n",
    "      \n",
    "        index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "      \n",
    "        return x.gather(dim, index)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embed(x)  # (N, W, D)\n",
    "\n",
    "        if self.args.static:\n",
    "\n",
    "            x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        \n",
    "        x = [self.kmax_pooling(i, 2, args.kmax).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "       \n",
    "        x = torch.cat(x, 1)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "\n",
    "        x = self.fc1(x)  # (N, C)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "\n",
    "        logit = self.fc2(x)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Mf3IS5B2_VJP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(train_iter, dev_iter, model, args):\n",
    "\n",
    "#    if args.cuda:\n",
    "\n",
    " #       model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    best_acc = 0\n",
    "\n",
    "    last_step = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, args.epochs+1):\n",
    "\n",
    "        for batch in train_iter:\n",
    "\n",
    "            feature, target = batch.text, batch.label\n",
    "\n",
    "            feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
    "\n",
    "            \n",
    "\n",
    "            feature, target = feature.cuda(), target.cuda()\n",
    "\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logit = model(feature)\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "            loss = F.cross_entropy(logit, target)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if steps % args.log_interval == 0:\n",
    "\n",
    "                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "\n",
    "                accuracy = 100.0 * corrects/batch.batch_size\n",
    "\n",
    "                sys.stdout.write(\n",
    "\n",
    "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n",
    "\n",
    "                                                                             loss.data[0], \n",
    "\n",
    "                                                                             accuracy,\n",
    "\n",
    "                                                                             corrects,\n",
    "\n",
    "                                                                             batch.batch_size))\n",
    "\n",
    "            if steps % args.test_interval == 0:\n",
    "\n",
    "                dev_acc = eval(dev_iter, model, args)\n",
    "\n",
    "                if dev_acc > best_acc:\n",
    "\n",
    "                    best_acc = dev_acc\n",
    "\n",
    "                    last_step = steps\n",
    "\n",
    "                    if args.save_best:\n",
    "\n",
    "                        save(model, args.save_dir, 'best', steps)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if steps - last_step >= args.early_stop:\n",
    "\n",
    "                        print('early stop by {} steps.'.format(args.early_stop))\n",
    "\n",
    "            elif steps % args.save_interval == 0:\n",
    "\n",
    "                save(model, args.save_dir, 'snapshot', steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval(data_iter, model, args):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    corrects, avg_loss = 0, 0\n",
    "\n",
    "    for batch in data_iter:\n",
    "\n",
    "        feature, target = batch.text, batch.label\n",
    "\n",
    "        feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
    "\n",
    "        feature, target = feature.cuda(), target.cuda()\n",
    "\n",
    "\n",
    "\n",
    "        logit = model(feature)\n",
    "\n",
    "        loss = F.cross_entropy(logit, target, size_average=False)\n",
    "\n",
    "\n",
    "\n",
    "        avg_loss += loss.data[0]\n",
    "\n",
    "        corrects += (torch.max(logit, 1)\n",
    "\n",
    "                     [1].view(target.size()).data == target.data).sum()\n",
    "\n",
    "\n",
    "\n",
    "    size = len(data_iter.dataset)\n",
    "\n",
    "    avg_loss /= size\n",
    "\n",
    "    accuracy = 100.0 * corrects/size\n",
    "\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "\n",
    "                                                                       accuracy, \n",
    "\n",
    "                                                                       corrects, \n",
    "\n",
    "                                                                       size))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(text, model, text_field, label_feild, cuda_flag):\n",
    "\n",
    "    assert isinstance(text, str)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    text = text_field.preprocess(text)\n",
    "\n",
    "    text = [[text_field.vocab.stoi[x] for x in text]]\n",
    "\n",
    "    x = text_field.tensor_type(text)\n",
    "\n",
    "    x = autograd.Variable(x, volatile=True)\n",
    "\n",
    "    if cuda_flag:\n",
    "\n",
    "        x = x.cuda()\n",
    "\n",
    "    print(x)\n",
    "\n",
    "    output = model(x)\n",
    "\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    return label_feild.vocab.itos[predicted.data[0][0]+1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save(model, save_dir, save_prefix, steps):\n",
    "\n",
    "    if not os.path.isdir(save_dir):\n",
    "\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4937,
     "output_extras": [
      {
       "item_id": 18
      },
      {
       "item_id": 19
      },
      {
       "item_id": 26
      },
      {
       "item_id": 27
      },
      {
       "item_id": 163
      },
      {
       "item_id": 244
      },
      {
       "item_id": 297
      },
      {
       "item_id": 357
      },
      {
       "item_id": 395
      },
      {
       "item_id": 433
      },
      {
       "item_id": 493
      },
      {
       "item_id": 500
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 289742,
     "status": "ok",
     "timestamp": 1520301374059,
     "user": {
      "displayName": "Ron Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101413017425979643796"
     },
     "user_tz": 480
    },
    "id": "vhQq0o7c_a5t",
    "outputId": "a44338c7-f4c1-46ba-c4d7-f51b251a979b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "Parameters:\n",
      "\t__DICT__=<attribute '__dict__' of 'args' objects>\n",
      "\t__DOC__=None\n",
      "\t__MODULE__=__main__\n",
      "\t__WEAKREF__=<attribute '__weakref__' of 'args' objects>\n",
      "\tBATCH_SIZE=128\n",
      "\tCLASS_NUM=2\n",
      "\tDEVICE=1\n",
      "\tDROPOUT=0.6\n",
      "\tEARLY_STOP=1000\n",
      "\tEMBED_DIM=256\n",
      "\tEMBED_NUM=19792\n",
      "\tEPOCHS=200\n",
      "\tG_OUT=100\n",
      "\tGRU_L=1\n",
      "\tKERNEL_NUM=150\n",
      "\tKERNEL_SIZES=[1, 2, 3, 4, 5]\n",
      "\tKMAX=3\n",
      "\tLOG_INTERVAL=1\n",
      "\tLR=0.001\n",
      "\tMAX_NORM=3.0\n",
      "\tNO_CUDA=False\n",
      "\tPREDICT=None\n",
      "\tSAVE_BEST=True\n",
      "\tSAVE_DIR=snapshot/2018-03-06_13-31-30\n",
      "\tSAVE_INTERVAL=500\n",
      "\tSHUFFLE=False\n",
      "\tSNAPSHOT=None\n",
      "\tSTATIC=False\n",
      "\tTEST=False\n",
      "\tTEST_INTERVAL=100\n",
      "\n",
      "Batch[71] - loss: 0.693117  acc: 57.0312%(73/128)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ron/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[100] - loss: 0.672384  acc: 53.9062%(69/128)\n",
      "Evaluation - loss: 0.674918  acc: 56.9755%(535/939) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ron/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:118: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[200] - loss: 0.579256  acc: 70.3125%(90/128)\n",
      "Evaluation - loss: 0.621235  acc: 64.2173%(603/939) \n",
      "\n",
      "Batch[300] - loss: 0.267434  acc: 88.2812%(113/128)\n",
      "Evaluation - loss: 0.649011  acc: 73.0564%(686/939) \n",
      "\n",
      "Batch[400] - loss: 0.054941  acc: 99.2188%(127/128))\n",
      "Evaluation - loss: 0.734123  acc: 76.4643%(718/939) \n",
      "\n",
      "Batch[500] - loss: 0.005936  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 0.842968  acc: 75.0799%(705/939) \n",
      "\n",
      "Batch[600] - loss: 0.001910  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 0.902478  acc: 75.6124%(710/939) \n",
      "\n",
      "Batch[700] - loss: 0.001319  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 0.956990  acc: 75.9318%(713/939) \n",
      "\n",
      "Batch[800] - loss: 0.000965  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.023293  acc: 75.9318%(713/939) \n",
      "\n",
      "Batch[900] - loss: 0.000477  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.109879  acc: 76.4643%(718/939) \n",
      "\n",
      "Batch[1000] - loss: 0.000143  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.201274  acc: 76.2513%(716/939) \n",
      "\n",
      "Batch[1100] - loss: 0.000167  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.274743  acc: 76.3578%(717/939) \n",
      "\n",
      "Batch[1200] - loss: 0.000146  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.334037  acc: 76.1448%(715/939) \n",
      "\n",
      "Batch[1300] - loss: 0.000063  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.382444  acc: 75.6124%(710/939) \n",
      "\n",
      "Batch[1400] - loss: 0.000027  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.424507  acc: 75.8253%(712/939) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1500] - loss: 0.000028  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.464335  acc: 76.1448%(715/939) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1600] - loss: 0.000031  acc: 100.0000%(128/128)\n",
      "Evaluation - loss: 1.500400  acc: 76.0383%(714/939) \n",
      "\n",
      "early stop by 1000 steps.\n",
      "Batch[1645] - loss: 0.000016  acc: 100.0000%(128/128)\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import argparse\n",
    "\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchtext.data as data\n",
    "\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "import torchtext\n",
    "\n",
    "\n",
    "class args():\n",
    "    lr=0.001\n",
    "    epochs=200\n",
    "    batch_size=128\n",
    "    log_interval=1\n",
    "    test_interval=100\n",
    "    save_interval=500\n",
    "    save_dir='snapshot'\n",
    "    early_stop=1000\n",
    "    save_best=True\n",
    "    shuffle=False\n",
    "    dropout=0.6\n",
    "    max_norm=3.0\n",
    "    embed_dim=256\n",
    "    kernel_num=150\n",
    "    kernel_sizes='1,2,3,4,5'\n",
    "    static=False\n",
    "    device=1\n",
    "    no_cuda=False\n",
    "    snapshot=None\n",
    "    predict=None\n",
    "    test=False\n",
    "    kmax=3\n",
    "    gru_l=1\n",
    "    g_out=100\n",
    "\n",
    "# load MR dataset\n",
    "\n",
    "def mr(text_field, label_field, **kargs):\n",
    "\n",
    "    train_data, dev_data = MR.splits(text_field, label_field)\n",
    "\n",
    "    text_field.build_vocab(train_data, dev_data)\n",
    "\n",
    "    label_field.build_vocab(train_data, dev_data)\n",
    "    \n",
    "    #text_field.build_vocab(train_data, dev_data ,torchtext.vocab.GloVe(name='6B',dim=200))\n",
    "\n",
    "    #label_field.build_vocab(train_data, dev_data ,torchtext.vocab.GloVe(name='6B',dim=200))\n",
    "\n",
    "    train_iter, dev_iter = data.Iterator.splits(\n",
    "\n",
    "                                (train_data, dev_data), \n",
    "\n",
    "                                batch_sizes=(args.batch_size, len(dev_data)),\n",
    "\n",
    "                                **kargs)\n",
    "\n",
    "    return train_iter, dev_iter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "\n",
    "label_field = data.Field(sequential=False)\n",
    "\n",
    "train_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# update args and print\n",
    "\n",
    "args.embed_num = len(text_field.vocab)\n",
    "\n",
    "args.class_num = len(label_field.vocab) - 1\n",
    "\n",
    "\n",
    "args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n",
    "\n",
    "args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "\n",
    "for attr, value in sorted(args.__dict__.items()):\n",
    "\n",
    "    print(\"\\t{}={}\".format(attr.upper(), value))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "\n",
    "cnn = CNN_Text(args).cuda()\n",
    "\n",
    "#init = text_field.vocab\n",
    "\n",
    "#cnn.embed.weight.data=init.vectors\n",
    "\n",
    "\n",
    "if args.snapshot is not None:\n",
    "\n",
    "    print('\\nLoading model from {}...'.format(args.snapshot))\n",
    "\n",
    "    cnn.load_state_dict(torch.load(args.snapshot))\n",
    "\n",
    "\n",
    "\n",
    "# train or predict\n",
    "\n",
    "if args.predict is not None:\n",
    "\n",
    "    label = train.predict(args.predict, cnn, text_field, label_field, args.cuda)\n",
    "\n",
    "    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n",
    "\n",
    "elif args.test:\n",
    "\n",
    "    try:\n",
    "\n",
    "        eval(test_iter, cnn, args) \n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "\n",
    "        train(train_iter, dev_iter, cnn, args)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        print('\\n' + '-' * 89)\n",
    "\n",
    "        print('Exiting from training early')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "CNN4Text(Kmax).ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
