{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN4Text(Kmax).ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bGV2j-qb_RN3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import os\n",
        "\n",
        "import random\n",
        "\n",
        "import tarfile\n",
        "\n",
        "import urllib\n",
        "\n",
        "from torchtext import data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TarDataset(data.Dataset):\n",
        "\n",
        "  \n",
        "\n",
        "    @classmethod\n",
        "\n",
        "    def download_or_unzip(cls, root):\n",
        "\n",
        "        path = os.path.join(root, cls.dirname)\n",
        "\n",
        "        if not os.path.isdir(path):\n",
        "\n",
        "            tpath = os.path.join(root, cls.filename)\n",
        "\n",
        "            if not os.path.isfile(tpath):\n",
        "\n",
        "                print('downloading')\n",
        "\n",
        "                urllib.request.urlretrieve(cls.url, tpath)\n",
        "\n",
        "            with tarfile.open(tpath, 'r') as tfile:\n",
        "\n",
        "                print('extracting')\n",
        "\n",
        "                tfile.extractall(root)\n",
        "\n",
        "        return os.path.join(path, '')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MR(TarDataset):\n",
        "\n",
        "\n",
        "\n",
        "    url = 'https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
        "\n",
        "    filename = 'rt-polaritydata.tar'\n",
        "\n",
        "    dirname = 'rt-polaritydata'\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "\n",
        "    def sort_key(ex):\n",
        "\n",
        "        return len(ex.text)\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
        "\n",
        "        \n",
        "\n",
        "        def clean_str(string):\n",
        "\n",
        "\n",
        "            string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "\n",
        "            string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "\n",
        "            string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "\n",
        "            string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "\n",
        "            string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "\n",
        "            string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "\n",
        "            string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "\n",
        "            string = re.sub(r\",\", \" , \", string)\n",
        "\n",
        "            string = re.sub(r\"!\", \" ! \", string)\n",
        "\n",
        "            string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "\n",
        "            string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "\n",
        "            string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "\n",
        "            string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "            return string.strip()\n",
        "\n",
        "\n",
        "\n",
        "        text_field.preprocessing = data.Pipeline(clean_str)\n",
        "\n",
        "        fields = [('text', text_field), ('label', label_field)]\n",
        "\n",
        "\n",
        "\n",
        "        if examples is None:\n",
        "\n",
        "            path = self.dirname if path is None else path\n",
        "\n",
        "            examples = []\n",
        "\n",
        "            with open(os.path.join(path, 'rt-polarity.neg'), errors='ignore') as f:\n",
        "\n",
        "                examples += [\n",
        "\n",
        "                    data.Example.fromlist([line, 'negative'], fields) for line in f]\n",
        "\n",
        "            with open(os.path.join(path, 'rt-polarity.pos'), errors='ignore') as f:\n",
        "\n",
        "                examples += [\n",
        "\n",
        "                    data.Example.fromlist([line, 'positive'], fields) for line in f]\n",
        "\n",
        "        super(MR, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "\n",
        "    def splits(cls, text_field, label_field, dev_ratio=.1, shuffle=True, root='.', **kwargs):\n",
        "\n",
        "        \n",
        "\n",
        "        path = cls.download_or_unzip(root)\n",
        "\n",
        "        examples = cls(text_field, label_field, path=path, **kwargs).examples\n",
        "\n",
        "        if shuffle: random.shuffle(examples)\n",
        "\n",
        "        dev_index = -1 * int(dev_ratio*len(examples))\n",
        "\n",
        "\n",
        "\n",
        "        return (cls(text_field, label_field, examples=examples[:dev_index]),\n",
        "\n",
        "                cls(text_field, label_field, examples=examples[dev_index:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvQKst2r_Sda",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CNN_Text(nn.Module):\n",
        "\n",
        "    \n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(CNN_Text, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        V = args.embed_num\n",
        "\n",
        "        D = args.embed_dim\n",
        "\n",
        "        C = args.class_num\n",
        "\n",
        "        Ci = 1\n",
        "\n",
        "        Co = args.kernel_num\n",
        "\n",
        "        Ks = args.kernel_sizes\n",
        "        \n",
        "\n",
        "       \n",
        "        self.embed = nn.Embedding(V, D)\n",
        "\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
        "\n",
        "        self.dropout = nn.Dropout(args.dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(len(Ks)*Co*args.kmax, 256)\n",
        "        \n",
        "        self.fc2 = nn.Linear(256, C)\n",
        "                \n",
        "        \n",
        "    def kmax_pooling(self, x, dim, k):\n",
        "      \n",
        "        index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
        "      \n",
        "        return x.gather(dim, index)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embed(x)  # (N, W, D)\n",
        "\n",
        "        if self.args.static:\n",
        "\n",
        "            x = Variable(x)\n",
        "\n",
        "        x = x.unsqueeze(1)  # (N, Ci, W, D)\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
        "        \n",
        "        x = [self.kmax_pooling(i, 2, args.kmax).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
        "       \n",
        "        x = torch.cat(x, 1)\n",
        "        \n",
        "        x = x.view(x.size(0),-1)\n",
        "\n",
        "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "\n",
        "        x = self.fc1(x)  # (N, C)\n",
        "        \n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
        "\n",
        "        logit = self.fc2(x)\n",
        "\n",
        "        return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mf3IS5B2_VJP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.autograd as autograd\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(train_iter, dev_iter, model, args):\n",
        "\n",
        "#    if args.cuda:\n",
        "\n",
        " #       model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    last_step = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "\n",
        "        for batch in train_iter:\n",
        "\n",
        "            feature, target = batch.text, batch.label\n",
        "\n",
        "            feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
        "\n",
        "            \n",
        "\n",
        "            feature, target = feature.cuda(), target.cuda()\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit = model(feature)\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "            loss = F.cross_entropy(logit, target)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "            if steps % args.log_interval == 0:\n",
        "\n",
        "                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "                accuracy = 100.0 * corrects/batch.batch_size\n",
        "\n",
        "                sys.stdout.write(\n",
        "\n",
        "                    '\\rBatch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(steps, \n",
        "\n",
        "                                                                             loss.data[0], \n",
        "\n",
        "                                                                             accuracy,\n",
        "\n",
        "                                                                             corrects,\n",
        "\n",
        "                                                                             batch.batch_size))\n",
        "\n",
        "            if steps % args.test_interval == 0:\n",
        "\n",
        "                dev_acc = eval(dev_iter, model, args)\n",
        "\n",
        "                if dev_acc > best_acc:\n",
        "\n",
        "                    best_acc = dev_acc\n",
        "\n",
        "                    last_step = steps\n",
        "\n",
        "                    if args.save_best:\n",
        "\n",
        "                        save(model, args.save_dir, 'best', steps)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    if steps - last_step >= args.early_stop:\n",
        "\n",
        "                        print('early stop by {} steps.'.format(args.early_stop))\n",
        "\n",
        "            elif steps % args.save_interval == 0:\n",
        "\n",
        "                save(model, args.save_dir, 'snapshot', steps)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval(data_iter, model, args):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    corrects, avg_loss = 0, 0\n",
        "\n",
        "    for batch in data_iter:\n",
        "\n",
        "        feature, target = batch.text, batch.label\n",
        "\n",
        "        feature.data.t_(), target.data.sub_(1)  # batch first, index align\n",
        "\n",
        "        feature, target = feature.cuda(), target.cuda()\n",
        "\n",
        "\n",
        "\n",
        "        logit = model(feature)\n",
        "\n",
        "        loss = F.cross_entropy(logit, target, size_average=False)\n",
        "\n",
        "\n",
        "\n",
        "        avg_loss += loss.data[0]\n",
        "\n",
        "        corrects += (torch.max(logit, 1)\n",
        "\n",
        "                     [1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "\n",
        "\n",
        "    size = len(data_iter.dataset)\n",
        "\n",
        "    avg_loss /= size\n",
        "\n",
        "    accuracy = 100.0 * corrects/size\n",
        "\n",
        "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
        "\n",
        "                                                                       accuracy, \n",
        "\n",
        "                                                                       corrects, \n",
        "\n",
        "                                                                       size))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict(text, model, text_field, label_feild, cuda_flag):\n",
        "\n",
        "    assert isinstance(text, str)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    text = text_field.preprocess(text)\n",
        "\n",
        "    text = [[text_field.vocab.stoi[x] for x in text]]\n",
        "\n",
        "    x = text_field.tensor_type(text)\n",
        "\n",
        "    x = autograd.Variable(x, volatile=True)\n",
        "\n",
        "    if cuda_flag:\n",
        "\n",
        "        x = x.cuda()\n",
        "\n",
        "    print(x)\n",
        "\n",
        "    output = model(x)\n",
        "\n",
        "    _, predicted = torch.max(output, 1)\n",
        "\n",
        "    return label_feild.vocab.itos[predicted.data[0][0]+1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save(model, save_dir, save_prefix, steps):\n",
        "\n",
        "    if not os.path.isdir(save_dir):\n",
        "\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    save_prefix = os.path.join(save_dir, save_prefix)\n",
        "\n",
        "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
        "\n",
        "    torch.save(model.state_dict(), save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vhQq0o7c_a5t",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 18
            },
            {
              "item_id": 19
            },
            {
              "item_id": 26
            },
            {
              "item_id": 27
            },
            {
              "item_id": 163
            },
            {
              "item_id": 244
            },
            {
              "item_id": 297
            },
            {
              "item_id": 357
            },
            {
              "item_id": 395
            },
            {
              "item_id": 433
            },
            {
              "item_id": 493
            },
            {
              "item_id": 500
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 4937
        },
        "outputId": "a44338c7-f4c1-46ba-c4d7-f51b251a979b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520301374059,
          "user_tz": 480,
          "elapsed": 289742,
          "user": {
            "displayName": "Ron Wang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "101413017425979643796"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import argparse\n",
        "\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "\n",
        "import torchtext.data as data\n",
        "\n",
        "import torchtext.datasets as datasets\n",
        "\n",
        "\n",
        "\n",
        "class args():\n",
        "  lr=0.001\n",
        "  epochs=200\n",
        "  batch_size=64\n",
        "  log_interval=1\n",
        "  test_interval=100\n",
        "  save_interval=500\n",
        "  save_dir='snapshot'\n",
        "  early_stop=1000\n",
        "  save_best=True\n",
        "  shuffle=False\n",
        "  dropout=0.6\n",
        "  max_norm=3.0\n",
        "  embed_dim=256\n",
        "  kernel_num=150\n",
        "  kernel_sizes='1,2,3,4,5'\n",
        "  static=False\n",
        "  device=1\n",
        "  no_cuda=False\n",
        "  snapshot=None\n",
        "  predict=None\n",
        "  test=False\n",
        "  kmax=5\n",
        "  gru_l=1\n",
        "  g_out=100\n",
        "\n",
        "# load MR dataset\n",
        "\n",
        "def mr(text_field, label_field, **kargs):\n",
        "\n",
        "    train_data, dev_data = MR.splits(text_field, label_field)\n",
        "\n",
        "    text_field.build_vocab(train_data, dev_data)\n",
        "\n",
        "    label_field.build_vocab(train_data, dev_data)\n",
        "\n",
        "    train_iter, dev_iter = data.Iterator.splits(\n",
        "\n",
        "                                (train_data, dev_data), \n",
        "\n",
        "                                batch_sizes=(args.batch_size, len(dev_data)),\n",
        "\n",
        "                                **kargs)\n",
        "\n",
        "    return train_iter, dev_iter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load data\n",
        "\n",
        "print(\"\\nLoading data...\")\n",
        "\n",
        "text_field = data.Field(lower=True)\n",
        "\n",
        "label_field = data.Field(sequential=False)\n",
        "\n",
        "train_iter, dev_iter = mr(text_field, label_field, device=-1, repeat=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# update args and print\n",
        "\n",
        "args.embed_num = len(text_field.vocab)\n",
        "\n",
        "args.class_num = len(label_field.vocab) - 1\n",
        "\n",
        "\n",
        "args.kernel_sizes = [int(k) for k in args.kernel_sizes.split(',')]\n",
        "\n",
        "args.save_dir = os.path.join(args.save_dir, datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nParameters:\")\n",
        "\n",
        "for attr, value in sorted(args.__dict__.items()):\n",
        "\n",
        "    print(\"\\t{}={}\".format(attr.upper(), value))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "cnn = CNN_Text(args).cuda()\n",
        "\n",
        "if args.snapshot is not None:\n",
        "\n",
        "    print('\\nLoading model from {}...'.format(args.snapshot))\n",
        "\n",
        "    cnn.load_state_dict(torch.load(args.snapshot))\n",
        "\n",
        "\n",
        "\n",
        "# train or predict\n",
        "\n",
        "if args.predict is not None:\n",
        "\n",
        "    label = train.predict(args.predict, cnn, text_field, label_field, args.cuda)\n",
        "\n",
        "    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n",
        "\n",
        "elif args.test:\n",
        "\n",
        "    try:\n",
        "\n",
        "        eval(test_iter, cnn, args) \n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(\"\\nSorry. The test dataset doesn't  exist.\\n\")\n",
        "\n",
        "else:\n",
        "\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "\n",
        "        train(train_iter, dev_iter, cnn, args)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "\n",
        "        print('\\n' + '-' * 89)\n",
        "\n",
        "        print('Exiting from training early')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading data...\n",
            "downloading\n",
            "extracting\n",
            "\n",
            "Parameters:\n",
            "\t__DICT__=<attribute '__dict__' of 'args' objects>\n",
            "\t__DOC__=None\n",
            "\t__MODULE__=__main__\n",
            "\t__WEAKREF__=<attribute '__weakref__' of 'args' objects>\n",
            "\tBATCH_SIZE=64\n",
            "\tCLASS_NUM=2\n",
            "\tDEVICE=1\n",
            "\tDROPOUT=0.6\n",
            "\tEARLY_STOP=1000\n",
            "\tEMBED_DIM=256\n",
            "\tEMBED_NUM=21114\n",
            "\tEPOCHS=200\n",
            "\tG_OUT=100\n",
            "\tGRU_L=1\n",
            "\tKERNEL_NUM=150\n",
            "\tKERNEL_SIZES=[1, 2, 3, 4, 5]\n",
            "\tKMAX=5\n",
            "\tLOG_INTERVAL=1\n",
            "\tLR=0.001\n",
            "\tMAX_NORM=3.0\n",
            "\tNO_CUDA=False\n",
            "\tPREDICT=None\n",
            "\tSAVE_BEST=True\n",
            "\tSAVE_DIR=snapshot/2018-03-06_01-51-29\n",
            "\tSAVE_INTERVAL=500\n",
            "\tSHUFFLE=False\n",
            "\tSNAPSHOT=None\n",
            "\tSTATIC=False\n",
            "\tTEST=False\n",
            "\tTEST_INTERVAL=100\n",
            "\n",
            "Batch[100] - loss: 0.685943  acc: 62.5000%(40/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:118: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation - loss: 0.690920  acc: 58.9118%(628/1066) \n",
            "\n",
            "Batch[153] - loss: 0.652398  acc: 68.7500%(44/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[200] - loss: 0.712865  acc: 59.3750%(38/64)\n",
            "Evaluation - loss: 0.660956  acc: 60.7880%(648/1066) \n",
            "\n",
            "Batch[300] - loss: 0.511425  acc: 80.0000%(48/60)\n",
            "Evaluation - loss: 0.613345  acc: 68.0113%(725/1066) \n",
            "\n",
            "Batch[400] - loss: 0.369413  acc: 84.3750%(54/64)\n",
            "Evaluation - loss: 0.602168  acc: 69.0432%(736/1066) \n",
            "\n",
            "Batch[500] - loss: 0.078572  acc: 98.4375%(63/64)\n",
            "Evaluation - loss: 0.748107  acc: 72.4203%(772/1066) \n",
            "\n",
            "Batch[600] - loss: 0.115526  acc: 96.6667%(58/60)\n",
            "Evaluation - loss: 0.765382  acc: 72.5141%(773/1066) \n",
            "\n",
            "Batch[700] - loss: 0.040771  acc: 98.4375%(63/64)\n",
            "Evaluation - loss: 1.047629  acc: 70.8255%(755/1066) \n",
            "\n",
            "Batch[800] - loss: 0.012803  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.051838  acc: 72.2326%(770/1066) \n",
            "\n",
            "Batch[900] - loss: 0.006100  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 1.177806  acc: 72.1388%(769/1066) \n",
            "\n",
            "Batch[1000] - loss: 0.001453  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.218774  acc: 73.2645%(781/1066) \n",
            "\n",
            "Batch[1100] - loss: 0.000504  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.227289  acc: 73.7336%(786/1066) \n",
            "\n",
            "Batch[1152] - loss: 0.000750  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[1200] - loss: 0.000373  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 1.318017  acc: 74.0150%(789/1066) \n",
            "\n",
            "Batch[1300] - loss: 0.000151  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.467761  acc: 74.0150%(789/1066) \n",
            "\n",
            "Batch[1400] - loss: 0.000054  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.529088  acc: 73.8274%(787/1066) \n",
            "\n",
            "Batch[1500] - loss: 0.000095  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 1.577717  acc: 73.7336%(786/1066) \n",
            "\n",
            "Batch[1600] - loss: 0.000054  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.620634  acc: 73.7336%(786/1066) \n",
            "\n",
            "Batch[1700] - loss: 0.000067  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.669822  acc: 73.6398%(785/1066) \n",
            "\n",
            "Batch[1800] - loss: 0.000048  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 1.754533  acc: 73.5460%(784/1066) \n",
            "\n",
            "Batch[1900] - loss: 0.000009  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.887764  acc: 74.0150%(789/1066) \n",
            "\n",
            "Batch[2000] - loss: 0.000032  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 1.935575  acc: 74.0150%(789/1066) \n",
            "\n",
            "Batch[2100] - loss: 0.000011  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 1.978331  acc: 74.2964%(792/1066) \n",
            "\n",
            "Batch[2146] - loss: 0.000019  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[2200] - loss: 0.000009  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.010963  acc: 74.2964%(792/1066) \n",
            "\n",
            "Batch[2300] - loss: 0.000010  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.047735  acc: 74.2026%(791/1066) \n",
            "\n",
            "Batch[2400] - loss: 0.000014  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.077862  acc: 74.1088%(790/1066) \n",
            "\n",
            "Batch[2500] - loss: 0.000003  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.101117  acc: 73.9212%(788/1066) \n",
            "\n",
            "Batch[2600] - loss: 0.000004  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.133798  acc: 73.8274%(787/1066) \n",
            "\n",
            "Batch[2700] - loss: 0.000004  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.157831  acc: 73.9212%(788/1066) \n",
            "\n",
            "Batch[2800] - loss: 0.000006  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.186112  acc: 73.8274%(787/1066) \n",
            "\n",
            "Batch[2900] - loss: 0.000002  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.208922  acc: 73.7336%(786/1066) \n",
            "\n",
            "Batch[3000] - loss: 0.000001  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.232794  acc: 73.6398%(785/1066) \n",
            "\n",
            "Batch[3100] - loss: 0.000006  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.247979  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3147] - loss: 0.000002  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[3200] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.276068  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3300] - loss: 0.000003  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.296630  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3400] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.314511  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3500] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.334125  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3600] - loss: 0.000001  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.352485  acc: 73.5460%(784/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3700] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.366911  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3800] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.391171  acc: 73.8274%(787/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[3900] - loss: 0.000001  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.403982  acc: 73.5460%(784/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4000] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.419927  acc: 73.8274%(787/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4047] - loss: 0.000001  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[4100] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.433500  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4200] - loss: 0.000002  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.456831  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4300] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.468175  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4400] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.483115  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4500] - loss: 0.000001  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.499802  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4600] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.513788  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4700] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.530430  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4800] - loss: 0.000001  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.539392  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4900] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.558843  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[4947] - loss: 0.000001  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[5000] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.566981  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5100] - loss: 0.000000  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.590375  acc: 73.5460%(784/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5200] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.603041  acc: 73.4522%(783/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5300] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.615876  acc: 73.5460%(784/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5400] - loss: 0.000000  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.622939  acc: 73.5460%(784/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5500] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.645176  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5600] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.655551  acc: 73.4522%(783/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5700] - loss: 0.000000  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.671806  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5800] - loss: 0.000001  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.683093  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[5847] - loss: 0.000000  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[5900] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.697144  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6000] - loss: 0.000000  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.712049  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6100] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.722213  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6200] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.735678  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6300] - loss: 0.000000  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.747677  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6400] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.759646  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6500] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.765352  acc: 73.5460%(784/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6600] - loss: 0.000000  acc: 100.0000%(60/60)\n",
            "Evaluation - loss: 2.783999  acc: 73.7336%(786/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6700] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "Evaluation - loss: 2.795561  acc: 73.6398%(785/1066) \n",
            "\n",
            "early stop by 1000 steps.\n",
            "Batch[6747] - loss: 0.000000  acc: 100.0000%(64/64)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch[6794] - loss: 0.000000  acc: 100.0000%(64/64)\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}